"1. Could you describe a specific instance where you successfully found one or more new dataset(s) that contributed to a data-driven task? What were you trying to achieve by searching for a new dataset? If you had any particular criteria for success, what were they? Please give as much detail as possible since the following questions will ask you to elaborate on this particular scenario.","2. In this particular scenario, from where did you obtain the new dataset(s)?","3. How did you assess the potential value of the dataset(s), considering whether it's worthwhile to download (or pay for if relevant) and work with?",Information Used,Data Meet Purpose,Data Quality Check,Pay Access Decision,(a) whether a dataset can fulfill your intended purpose,(b) if data is of sufficient quality,(c) if data is worth paying some price to access?,"1. Could you describe a specific instance where you were unsuccessful in the sense that the datasets you found could not contribute to a data-driven task? What were you trying to achieve by searching for a new dataset? What were the challenges that led to this outcome? Please give as much detail as possible since the following questions will ask you to elaborate on this particular scenario. 
","2. In this particular scenario, from where did you obtain the new dataset(s)?","3. How did you assess the potential value of the dataset(s), considering whether it's worthwhile to download (or pay for if relevant) and work with?",Information Used_N,Data Meet Purpose_N,Data Quality Check_N,Pay Access Decision_N,(a) whether a dataset can fulfill your intended purpose,(b) if data is of sufficient quality,(c) if data is worth paying some price to access?
"I work in a team that builds a distributed system. Once I found the availability of our system dropped significantly, and I was responsible to fix it asap. I looked all over the place to find out what's wrong. Eventually, I found one dataset that showed machines having abnormal cpu usage, and another dataset that showed the abnormal cpu usage occurred right after the latest push. These info give me the confidence that the inavailability issue is caused by a bad push. So I reverted the push and the availability went back to normal.",team's developer wiki,The dataset is a must-have to debug crisis like this. I will download or pay for if I have to.,"User reviews, Metadata, Data collection conditions, Visualizations",Yes,Yes,Yes,"I believe in user reviews; I can lean on real examples from users/developers to know whether the dataset fits my usecase. Metadata and visualizations will give me a more concrete idea of what the dataset is about, thus I can decide whether it fulfills my purpose.",Data collection conditions will let me know whether the dataset is usable or up-to-date.,"If a quality dataset fits my purpose, I will pay for it.",I was interested in Chinese stock markets and want to find more macroeconomic financing data. But I couldn't find any with reliable source or up-to-date information.,Google,"Very high, I'm willing to download and pay for it.","Source reputation, Metadata, Sample data",Yes,Yes,Yes,Metadata and sample data will give me confidence if the dataset is useful.,I rely on source reputation to decide if the data is reliable,"If a quality dataset fits my need, I'm willing to pay."
"For my data management coursework project, I need to find datasets containing the poverty rate and other demographic data in New York City, the City of Los Angeles, and Chicago at the neighborhood/community level. The challenge is most official datasets released by local governments with my data of interest are at the census tract level or at the zip code level. So the level of analysis does not match. It took me some time to finally obtain, both by downloading and scraping websites, to successfully get datasets from reliable sources.","NY, LA, and Chicago's local open data platforms; state-level think tank (e.g. Illinois Policy); national-level think tank (e.g. Economic Innovation Group); open-source data websites (e.g. City-Data) updated by volunteers annually","The deciding factors for me is the data source and whether there is a clear rules for data wrangling if I am looking for a processed dataset. For example, one of the reliable sources for U.S. poverty rates is U.S. National Census Bureau. But their data system is not the most intuitive for obtaining data. So when looking for a processed dataset, I checked the data source and whether they specified somewhere in the dataset/visualization about how they processed the original data. A great example would be this map showcasing poverty by city and community in LA county: https://www.arcgis.com/apps/dashboards/7846c3c37dff4728923609a9f55f849c. On the data disclaimer section (bottom left), they specify a reliable source for data and their data processing rules for calculating the poverty rate of each neighborhood in LA county. The combination of the two pieces information is essential for me to evaluate whether a dataset is worthwhile to download and work with. 

Another important factor is, even though the map maker did not upload the processed dataset, the website is open for scraping, making obtaining the poverty rate data possible.","Source reputation, Sample data, Clear data wrangling rules for processed datasets",Yes,Yes,Didn't need to pay,"The source will be adequate if it is authoritative and commonly used by other researchers. Given that poverty rate is a relevant concepts for researchers across social science fields, the dataset should at least be commonly used in other peer-reviewed journals or is compiled by local/national governments who have the most reliable access to collect the information.","Again, the data source should be adequate if they satisfy the conditions above. Sample data will be sufficient to demonstrate data qualify if they showcase the name of columns (variables) and the recorded values for each column.",Right source; Clear data processing rules (or codebook); Maintained and updated on a regular basis; Allow downloading sample data.,"I will still use the poverty rate example. I tried all official government sources when trying to find the poverty rate data for City of Los Angeles at the neighborhood level. The official datasets are either at the level of county or census tract, meaning the unit of analysis is either too big or too small for my project.","I encountered a map post on Arcgis. It is a map visualizing poverty by city and community in LA county, and one of its layer contains poverty rate at the neighborhood level, which is just what I wanted.",Same as before. The source of the data and whether the data processing rules are clear.,"Source reputation, Ability to access full datasets, Clear data processing rules",Yes,Yes,Didn't need to pay,"I check whether the level of analysis is what I wanted - at the neighborhood level, not the county level or the census tract level.","Same as before. Depending on whether the data source is authoritative, whether the data processing rules are clear, and perhaps whether it is commonly used by other researchers studying the same research topic.",Answer same as before.
"Having worked on language models, I regularly need to find datasets for tasks that are tailored to my application. Usually, the dataset is used for the supervised training of models, or the evaluation of them as well. To do so, I have a few different strategies: 

Automated Searches: I developed an internal tool for Slack that allows me to search using natural language. This tool helps me find datasets by understanding my specific needs and finding relevant research papers that include links to their datasets. In addition, they disclose information such as dataset structure and content.

Looking Through Conference Papers: I checked out papers from major conferences in language processing and linguistics. These papers often introduce new datasets and are a great source of high-quality data.

Exploring Online Repositories: I also explored online platforms that collect datasets for language tasks, such as the Linguistic Data Consortium or Hugging Face's dataset library. These sites have a wide variety of datasets ready to use.

Usually, I’ll use the noted automated tool to accelerate my process of exploring new datasets, with requirements I’m searching for. My largest concern is with discovery, and usually the time required to search for them.",Hosted on GitHub.,"1. Content - Does it contain the required details and content I'm looking for. 
2. Quality - Is the quality of the data up to par.

Usually, once a dataset is discovered, investigating it requires little to no incentive. ","Source reputation, Ability to access full datasets, Sample data, Visualizations",Yes,Yes,Didn't need to pay,"Source Reputation - Roughly indicative of the quality of data. Usually is peer-reviewed.
Access: Need to access dataset, to use it. Therefore, it’s a requirement.Sample Data: Quick representation of the type of data I’m looking for.Visualizations: Depends on the kind of visualization. If certain metrics such as IAA are easily viewable, then I would appreciate it.","Source Reputation is usually indicative of this. However, standard scores such as IAA can reaffirm the assumption.","There is so much data available. It's usually not worthwhile. 

If it was a niche dataset that I have to pay for, I would question if I should just create my own data.","In one particular instance, my goal was to enhance the accuracy of a machine learning model designed to predict consumer behavior in emerging markets, specifically focusing on online shopping patterns. The task required granular, up-to-date datasets reflecting consumer interactions, preferences, and trends across different online platforms within these markets.

I employed a similar strategy as before, combining automated tools for scanning through academic papers and datasets, and manual exploration of dataset repositories and conference proceedings. My focus was on finding datasets that included:

E-commerce transactions and user behavior data in emerging markets.
Social media analytics related to shopping patterns.
Demographic and psychographic profiles of online shoppers.

Despite a comprehensive search, the datasets I discovered faced several limitations that rendered them unsuitable for the task at hand:

Outdated Information: Many of the datasets were outdated, reflecting consumer behavior from several years ago, which is not representative of current trends due to the rapid evolution of online shopping habits.

Lack of Specificity to Emerging Markets: I found numerous datasets on consumer behavior, but they predominantly focused on established markets with very few insights into emerging markets. This lack of specificity made it challenging to derive meaningful insights for the target demographic.

Data Privacy and Ethics Concerns: A few promising datasets raised concerns regarding data privacy and ethical use, lacking clear consent from users for the use of their data in such analyses.",NA - Could not find new dataset. Had to create own dataset.,"Prioritized my requirements, and searched for all datasets matching description. Due to the lack of datasets in this area, I considered it worthwhile to explore tangent datasets that were similar to the ones I was looking for.","Source reputation, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data, Visualizations",Yes,Yes,Didn't need to pay,Has the required content.,Source of information + manual evaluation.,"NA - Did not pay for data. But, lack of data incentivized paying for the annotation of data."
"We had developed a visualization tool to help evaluate a machine learning model, and needed various datasets to benchmark and test our system, and for including public demos for a publication. We found a specific dataset of question-answering pairs of text that was a great example to use.",We found this data from a previously published paper that open-sourced their data. This dataset was relatively well known and highly cited.,"The reputation of the dataset is widely known, and one of our collaborators had used it before. So we knew from word of mouth that the data was sufficient. ","Source reputation, User reviews, Metadata, Ability to access full datasets, Sample data",Yes,Yes,Didn't need to pay,The full dataset was available and had data samples on the website. That was useful for looking at the format but also what to expect for each data point.,"While we did not know the overall quality of the set on the surface, our collaborator had previously worked with this data and could vouch for it's utility. ","Not relevant, open-sourced as part of a research paper. ","I was asked to help evaluate a machine learning model to catch potential biases and regions of input where the model may not be as robust or produce undesirable predictions. We need to find targeted datasets to evaluate specific behaviors. Strategies included using existing data or synthetically generating new data. Both have their pros and cons. We ultimately tried both, but first approach to find an external datasets that loosely matched what we were going for.",We found this data from an external company who had collected a specific dataset and released it as part of an open-source resource with an accompany research paper.,"Since the data was free, value was assessed by first reading the paper and understand how this data was collected, and then we processed it into a easily parsable form in a python notebook. From there, we could scroll through the data one by one, but for better summarization we built lightweight visualizations to get a sense of the full dataset. Examples include histograms for univariate features, hierarchical clustering, topic modeling, and embedding visualizations. This helped us get a qualitative sense of what was in the data and if it were to be useful. ","Source reputation, Metadata, Data collection conditions, Ability to access full datasets",Yes,No,Didn't need to pay,"The dataset was specifically made to help evaluate the behavior of the machine learning model I had, so I knew it would be relevant. However, there was no info about data coverage or in-data distribution.","The provided information did not show many examples or have any summary of what was to be included. While the data was reasonably formatted, there was no guarantee if it satisfied our problem. For example, if a dataset was advertised has having 100 data points, but 99 of them are practically the same data point just nudged a bit, you only have ""2"" data points. This was the case in this example. Useful for starting, but not for final use.","Not relevant, open-sourced as part of a research paper. "
"Most of the time, we use internal data though often augmented from census or other government sources. 

Often have used more public domain data sets (R, UC Irvine,  kaggle, census, ACS) for demonstrating tool usage. 

Created models and analysis of COVID rates during initial parts of the pandemic - found models from JHU health data. 

One example to drill down on is the demonstration of explainable models (Explainable Boosted Models or GAMs) where we used several datasets from UCIrvine repository and Kaggle (Boston real estate, Ames Iowa real estate, King County Real Estate, Wine, Heart Disease). Demonstration is public at https://microsoft.github.io/msrgamut/)",UC Irvine for most of the datasets,Was most worried about finding datasets with easy licensing for public consumption.,"Source reputation, Ability to access full datasets",Yes,Yes,No,"When building demonstrations, data needs to be well understood by wide audience (so special expertise should not be necessary). They should also be publicly available without burdensome license conditions.",Yes,No,"Many times it is difficult to find data at appropriate resolution. One instance was in building visualizations for storytelling about elections where we wanted precinct level data. Was easy to find county level data, but insufficient for our purposes. Similarly, cleaning the census data for that resolution (and 'joining' them since the data was from different geographic levels) was difficult.","Did the best we could, with the county level data.","Again, given that we're focused on public demonstrations of tools, we tend not to pay for datasets. Alternatively, we often develop tools on internal data that can not be shown externally so finding comparative, public data is desirable but difficult. ","Source reputation, Metadata, Ability to access full datasets, Matching the kind of data that we want to focus on the tool (for instance, log data at right granularity/cardinality of properties), model data with appropriate complexity and ability to fit well.",Yes,Yes,Didn't need to pay,Sufficient,"Since we didn't pay, usually could download and assess.","Potentially, but given above issues, not typically."
"I am developing a visualization library called Mascot.js, which is accompanied by a gallery of visualizations to demonstrate what the library can do. Currently the gallery has more than 60 visualizations, each of them has an underlying dataset. ","Datasets from the galleries of existing visualization languages and authoring tools, e.g., d3.js, Vega-Lite, Data Illustrator (my previous project) ","I start by thinking about what visualizations I want to add to the gallery, and then I decide the details of visualization designs, which determine what dataset(s) are appropriate.","Source reputation, Metadata, Ability to access full datasets, Sample data, Visualizations",Not Sure,Yes,Didn't need to pay,"I really need to see the full dataset, not only in a tabular format, but actually to visualize it so determine if it is really appropriate for the visualization design I want to add to the gallery. ",I think the source reputation and sample data can help me determine if the data is of sufficient quality. ,I didn't pay to access those datasets. ,"I wanted to find some interesting tree/hierarchical datasets for tree visualization demos, and I asked a student to search for such datasets. The hope was that the datasets should have multi-attribute nodes, so that I can show how these attributes can be aggregated or visually encoded. In addition, I want the topics/domains of these datasets to be more accessible to the general public, since tree datasets used in visualization research often come from technical domains like computer science or biology. We didn't find good datasets matching our expectation. ",Mostly google search. ,We downloaded the datasets to review them. ,"Source reputation, Metadata, Ability to access full datasets, Sample data, Visualizations",No,Not Sure,Didn't need to pay,I need to see the full dataset to determine if it meets my expectation. ,Source reputation and sample data should be sufficient. ,I didn't need/want to pay. 
"There were many sources, from governments' websites to competition platforms like Kaggle. However, our goal was to find a meaningful dataset for a specific purpose, such as understanding a social behavior or environmental impact, rather than merely the score of movie reviews on IMDB. This became the main criterion for us when selecting datasets.",https://data.fivethirtyeight.com/,"The website I shared above has an info button for each dataset that tells you more about how this dataset was initially gathered. Based on this information, I prefer the dataset with a more legitimate description and source.","Source reputation, Metadata, Data collection conditions, Ability to access full datasets",No,No,No,I can only assess this once I actually try out the dataset.,"Similarly, I can only assess this once I actually try out the dataset.","It depends on the price as well. If the dataset is affordable, I don't mind purchasing it even though the information is imperfect.","I hoped to find data about how plastic waste was handled in our state and the United States. However, this information is scattered across different websites and is not useable in many ways, such as by having an incomplete description of the data source.",I tried to find it on the governments' websites but failed.,"I first assessed the datasets based on their sources, which turned out to be flawed. As a result, I didn't continue further assessment.",Source reputation,No,No,No,"Similar to the previous example, I believe that I can only understand this after trying out the dataset.",^ Same as the previous answer,"Again, the amount of price might affect this answer. If the price is affordable, I don't mind paying it to get access."
"In a recent demand forecasting project within our sector, we aimed to enhance our predictive accuracy by integrating broader market data. We identified a critical resource in NielsenIQ's dataset, which provided detailed sales data on our competitors. This external dataset was pivotal in offering insights beyond our internal metrics, enabling a comprehensive market analysis. After standardizing the NielsenIQ data to match our internal formats, we used advanced analytics to incorporate market dynamics into our forecasting models. This enriched analysis led to improved forecasting accuracy, informed our inventory strategies, and identified competitive positioning opportunities.",NielsenIQ,"To assess the potential value of the NielsenIQ dataset, we first considered its alignment with our specific needs for enhancing demand forecasting within our industry. We evaluated the dataset's coverage, specifically looking for comprehensive sales data on competitors that could fill gaps in our market understanding. Additionally, we conducted a cost-benefit analysis to weigh the dataset's cost against the expected benefits, such as improved forecasting accuracy, better strategic positioning, and potential for increased market share. This evaluation involved consulting with our analytics team to estimate the potential impact on our forecasting models and decision-making processes, ensuring that the investment in the dataset would provide actionable insights and a significant return on investment.","Source reputation, Metadata, Data collection conditions, Prevalence of missing data, Sample data, Visualizations",Yes,Not Sure,Not Sure,"The provided information was deemed adequate for several reasons:
1. Source Reputation: The credibility of NielsenIQ as a reputable data provider assured us of the dataset's reliability.
2. Metadata: Detailed metadata confirmed the dataset’s relevance and scope matched our project needs.
3. Data Collection Conditions: Insights into the dataset’s creation process ensured its applicability to our forecasting models.
4. Sample Data: Preliminary examination through sample data affirmed the dataset's integration potential with our systems.
Overall, these points made us confident the dataset was up to the task.","The provided information was helpful but left me uncertain about the dataset's quality due to two main issues:
1. Missing Data: The heads-up on missing data was useful, but it raised concerns about how these gaps might affect our analysis.
2. Geographic Coverage: The lack of clear info on whether the dataset covers all our needed geographic locations added to this uncertainty.","Without comprehensive insights into these critical areas, conducting a precise cost-benefit analysis becomes challenging. This makes me unsure if the investment would guarantee the expected return, especially in meeting our specific geographic and data completeness criteria.","Our objective was to identify emerging food trends and shifting consumer preferences to inform the development of new products in the food processing industry. To achieve this, we sought to analyze diverse datasets encompassing consumer feedback and market analysis reports. We sourced datasets mainly from NielsenIQ. Despite the potential these datasets appeared to offer, they ultimately fell short of providing the actionable insights needed for our new product development project. The lack of timely, relevant, and specific data led to inconclusive results, making it challenging to pinpoint emerging food trends accurately.",NielsenIQ,"Here's how we assess the potential value of the datasets:
1. Project Relevance: Checked if data topics matched our goals of capturing consumer habits and food trends.
2. Quality & Completeness: Looked for recent, reliable data with minimal missing information to ensure accuracy.
3. Geographic Relevance: Prioritized datasets with detailed location data matching our target markets for localized insights.
4. Sample Review: Examined sample data for compatibility with our systems and ease of analysis.
5. Stakeholder Input: Consulted with product teams and analysts to confirm the dataset's utility and applicability.","Source reputation, Metadata, Data collection conditions, Prevalence of missing data, Sample data, Visualizations",Not Sure,No,Not Sure,"While the dataset covered relevant topics, the descriptions lacked the specificity needed to determine how closely the data matched our exact research questions. Details on the depth of the data (e.g., the granularity of consumer preferences or the breadth of food trends covered) were insufficient.","Information on the recency of the data, its completeness, and the methodology used for data collection was vague or missing. This raised concerns about the dataset's reliability and current relevance, which are critical for making informed decisions in product development. 
The information provided did not specify the geographic focus of the data. Our project requires data pertinent to specific regions or markets, and without this detail, we couldn’t ascertain the dataset's applicability to our target demographics.","Lack of detailed visualizations hindered our ability to preliminarily assess the dataset's format, quality, and how easily it could be integrated with our existing data infrastructure."
Looking for a medical imaging dataset with patient demographic information,From a colleague researcher,"Publicly available, download was free","Source reputation, Metadata, Data collection conditions, Sample data",Yes,Yes,Didn't need to pay,adequate,adequate,adequate,"I found a mammography dataset but all the images were from the same scan, I needed images from different scans to build a model with better robustness and generalizability",I read about it in a paper,"It was publicly available, so I downloaded it","Source reputation, Ability to access full datasets",No,No,Didn't need to pay,inadequate,adequate,didn't pay
"I will first search for the dataset in Google (eg. PSG public dataset), and then I will quickly get an overview of where this kind of datasets are stored and shared, then I will check the description or collect them to a form to choose which dataset I want to use.","PhysioNet, NSRR, Paperwithcode",with the description of the data ,"Source reputation, Metadata, Data collection conditions, Ability to access full datasets",No,No,Didn't need to pay,"usually I'll check in what purpose this dataset was collected, in medical studies they often have the test group and control group, and for machine learning purpose I will check if there's the ideal label I want to work with. usually the distribution of data can also help.","very hard to check, if the data is often used in similar studies they should have a baseline quality, if its a new dataset I have to check it myself. but I think data are by default dirty and need to be cleaned","it depends on the funding I have, if they are paid data I will first ask for some demo data.","usually most of the dataset I found cannot serve my needs, but I will collect them for future needs, so they either have been designed for different purpose, or they are not properly labelled or there're not enough data points to train to converge",PhysioNet,"with all the metadata I can get, also in free data I just download them and then check if they can be useful.","Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets",No,No,Didn't need to pay,try use the data ,"do the quality check, see if there's missing data or out of distributions.",have no experience.
"We needed to identify the effect of a policy. While we could have conducted our own survey, we found other survey microdata that allowed us to answer the question. In our case, the survey asked specific questions that were of interest. ",from a survey company that fielded the proprietary survey.,"We had the company send us the questionnaire, code book, and methodology. They also provided us with a sample section from a previous year's report that they had produced. From these items, we were able to assess the questions asked and whether they met our needs. Importantly, the methodology allowed us to determine whether there was strong selection bias or non-response that could have affected our inference. We determined that the dataset met our needs, so we purchased it.","Source reputation, Metadata, Data collection conditions, Ability to access full datasets, Visualizations, Sample section from previous year's report",Yes,Yes,Yes,The questionnaire allowed us to see the exact wording of the questions being asked and the order in which items and responses were presented. From this we could clearly see that the questions would provide information we needed to inform our policy development.,"The questionnaire, code book, and methodology allowed us to see the distribution of responses and determine whether the data appeared to be of high quality (we would not know for sure until we received the microdata). Had the questions been poorly worded, there been large amounts of missing data (indicated by the responses in the code book), or the methodology been weak or unavailable, it would have suggested to us that the data was low quality.","Based on the quality of the questionnaire, codebook, and methodology, we determined that the data was likely of high quality and worth the price of purchase.","While working on another project in a previous role, we looked for other proprietary data that we could purchase. Unfortunately, the few sources we found were full of missing or incorrect data. Some information was wrong or duplicated, there was incorrect spelling, formatting was atrocious. Very few sources collect the information we were looking for, so there were few options to get usable data.",Proprietary vendor,"We couldn't assess the quality until we had purchased the data. However, given how few sources collected the data, we had to gamble on whether the data was worth the price.","Source reputation, Data collection conditions, Sample data",No,No,Yes,"We knew exactly what type of data we were looking for. If that data had been high quality, it would have allowed us to answer our question.","We had to rely on the reputation of the chosen vendor to determine whether the data would have been in good quality. There was some sample data, but not enough to determine if the full dataset would meet our needs. As the data set was proprietary, there was little we could do to verify if the full data set would meet our needs until after purchase.","We were provided sample data. As we were unable to determine from the sample data if the data would meet our needs, we did not purchase it."
"I was working on scalable data visualizations and was looking for a dataset that would be relatable for a large audience, large enough that it challenges existing visualization tools, but small enough that it fits on my machine. ",https://sci.esa.int/web/gaia/,I downloaded a sample and evaluated it. Then I wrote a script to grab the full dataset and merge the pieces.,"Ability to access full datasets, Sample data",Yes,Yes,Didn't need to pay,The samples were similar enough to the full dataset. They covered a specific section of the sky so it wasn't just a random sample. Note that the sample was basically the beginning of the dataset (I downloaded the first archive). ,"I could visualize the dataset. Again, it was not a random sample which was important.",na,I can't really think of a case where I found data but didn't find it useful. Most of the time the datasets are not public/usable because the format sucks (e.g. PDF). Maybe an example is when I was looking at air quality data in Pittsburgh. ,https://www.alleghenycounty.us/Services/Health-Department/Air-Quality,I tried to get the data but there was no API (tableau public). ,"Source reputation, Ability to access full datasets",No,No,Didn't need to pay,I couldn't even access the dataset. ,I couldn't even access the dataset. ,na
"We recently leveraged data from the monitoring the future study which is a cohort-based, cross sectional survey of life and work values. We were looking for external data on this topic which offered a long historical perspective. Our criteria were that the data were nationally representative and covered the age range we were looking for. We also wanted something that was in common use and had been evaluated by third parties/was a well known and well documented data source.",From the university of Michigan website which maintains the data repo.,Please see my answer above,"User reviews, Metadata, Data collection conditions, Ability to access full datasets",Yes,Yes,Didn't need to pay,The documentation provided ample details as to the fit of the data to our needs.,Here we relied on outside reputation and historical references by peers.,This is usually determined by need. Usually the data that I need to purchase are only available from one provider.,"Moving out of my current role for the government and as an adjunct faculty member, I was looking for a synthetic psychometric dataset, but couldn’t find one that met all my interests. I wanted something that students could conduct reliability, validity, and adverse impact analyses, but all the data were synthetic and met certain parameters. Specifically, I wanted their to be adverse impact when only using a job knowledge test to predict job performance but no adverse impact when using a battery which added an interview, personality assessment, and tenure. I ended up simulating the data I wanted instead of finding it elsewhere.",See above,"It had to meet the conditions above. I tried some other sources, I believe there’s something called “Human Resources dataset” on Maggie but it wasn’t sophisticated enough.","Source reputation, User reviews, Metadata, Data collection conditions, Ability to access full datasets",Yes,Yes,Didn't need to pay,See above ,See above,Did not need to pay in this instance
"I was looking for datasets containing the usage events of our companies products and some other informational tables that can be joined together. 
Where: my companies private storages
Criteria of success: whether it contains some raw data what i’m looking for
",companies private storages,"By sacnning the first few records of the data + 
I also found that other people uses those data for similar purposes.","Source reputation, Sample data",Yes,Yes,Didn't need to pay,Sampled data has the columns that I want,Sampled data’s some aggregated measures are reasonable.,No needed to pay,The dataset that I found was too large to query/analyze. ,Company’s private storage,Saw many other people uses those datasets,Ability to access full datasets,Yes,Not Sure,Didn't need to pay,"After running some sampling codes, I was able to determine that this is too large to go ahead.",Cannot determine its quality since i was not able to access it really well.,no needed to pay
"One specific instance where I successfully found a new dataset that contributed to a data-driven task was when we were analyzing the usage patterns of our electric vehicle charging stations in urban areas. We wanted to understand peak usage times, average charging durations, and the types of vehicles being charged to optimize our charging infrastructure. The criteria for success in finding this dataset included relevance to EV charging patterns, data reliability, and sufficient granularity to extract meaningful insights.","In this scenario, I obtained the new dataset from a reputable transportation research organization that collects data on EV charging stations' usage across different cities","To assess the potential value of the dataset, I considered several factors. These included the reputation of the data source, metadata detailing the data collection methodology, prevalence of missing data, and the ability to access the full dataset rather than just sample data.","Source reputation, Metadata, Ability to access full datasets",Yes,Yes,Yes,"The provided information was adequate for assessing whether a dataset can fulfill our intended purpose because it aligned with our criteria for relevance, reliability, and granularity.",The provided information was adequate for assessing if data is of sufficient quality because it included details about data collection conditions and the prevalence of missing data.,The provided information was adequate for assessing if data is worth paying some price to access because it covered important aspects such as data quality and relevance to our needs.,"In one instance, I was unsuccessful in finding datasets that could contribute to a data-driven task related to analyzing the environmental impact of our waste-to-energy operations. We wanted to gather data on emissions, energy efficiency, and waste reduction metrics to assess our environmental performance. The challenges that led to this outcome included a lack of available datasets with relevant and comprehensive environmental impact data for our specific operations.","In this scenario, I attempted to obtain new datasets from various environmental agencies, industry research organizations, and government databases that typically house such data.","I assessed the potential value of the datasets by considering factors such as source reputation, metadata detailing data collection conditions, prevalence of missing data, and the ability to access the full datasets rather than just sample data.","Source reputation, Metadata, Prevalence of missing data",No,No,Not Sure,The provided information was inadequate for assessing whether a dataset can fulfill our intended purpose because we could not find datasets with the necessary environmental impact metrics,The provided information was inadequate for assessing if data is of sufficient quality because we could not find datasets with comprehensive and reliable environmental impact data.,N/A since we did not find a dataset that required payment.
Was looking for performance metrics for hospitals to check against existing client base and looking for potential new clients,data.gov,Dataset and column names' relevance,"Source reputation, Metadata, Ability to access full datasets, Sample data",Yes,Not Sure,Didn't need to pay,Metadata descriptions looked promising,Government is a reputable data source,Didn't need to pay,No,N/A,N/A,N/A,Not Sure,Not Sure,Didn't need to pay,N/A,N/A,N/A
"We were analyzing customer preferences and trends in street food consumption to optimize our product offerings and marketing strategies. The goal was to understand which local foods were most popular among customers, their purchasing patterns, and the factors influencing their food choices.","obtained the new dataset from a market research firm specializing in food and beverage consumer trends. The dataset included detailed information on customer preferences, buying behavior, demographic profiles, and geographical distribution.","To assess the potential value of the dataset, we considered factors such as source reputation, metadata describing data collection conditions, prevalence of missing data, and the ability to access the full dataset rather than just sample data. We also evaluated the relevance of the data to our specific research goals and the potential impact it could have on our decision-making processes.","Source reputation, User reviews, Metadata, Data collection conditions, Ability to access full datasets, Sample data",Yes,Yes,Yes,"The provided information was adequate for assessing whether a dataset can fulfill our intended purpose because it aligned with our criteria for relevance, reliability, and granularity in understanding customer preferences and trends.","The provided information was adequate for assessing if data is of sufficient quality because it included details about data collection conditions, source reputation, and the prevalence of missing data, which are crucial factors in determining data quality.","The provided information was adequate for assessing if data is worth paying some price to access because it covered important aspects such as data quality, relevance to our research goals, and potential impact on our decision-making processes.","In one instance, we were unsuccessful in finding datasets that could contribute to a data-driven task related to analyzing customer preferences for street food in specific geographical areas. We aimed to understand regional variations in food choices and preferences to tailor our product offerings and marketing strategies accordingly. The challenges that led to this outcome included a lack of available datasets with detailed and granular data on street food consumption patterns across different regions.","We attempted to obtain new datasets from market research firms, government agencies, and industry reports specializing in food consumption trends and regional preferences.","To assess the potential value of the datasets, we considered factors such as source reputation, metadata describing data collection conditions, prevalence of missing data, and the ability to access the full dataset rather than just sample data. We also evaluated the relevance of the data to our specific research goals and the potential impact it could have on our decision-making processes.","Source reputation, User reviews, Metadata, Sample data",No,No,Not Sure,The provided information was inadequate for assessing whether a dataset can fulfill our intended purpose because we could not find datasets with the necessary granularity and regional specificity in street food consumption patterns.,The provided information was inadequate for assessing if data is of sufficient quality because we could not find datasets with detailed and reliable data on street food preferences across different regions.,N/A since we did not find a dataset that required payment.
"One specific instance where I successfully found a new dataset that contributed to a data-driven task was when we were analyzing plastic waste composition and recycling rates in specific regions of Nigeria. We aimed to understand the types and quantities of plastic waste generated, the feasibility of recycling different types of plastics, and the potential impact on our production processes and market strategies.","The new dataset was obtained from environmental agencies, waste management reports, and recycling facilities in the targeted regions. These sources provided detailed data on plastic waste collection, sorting, recycling rates, and material recovery.","We assessed the potential value of the dataset by considering factors such as data collection conditions, data reliability, relevance to our research goals, and the ability to extract actionable insights to improve our waste management and recycling initiatives.","Metadata, Data collection conditions, Prevalence of missing data, Sample data, Visualizations",Yes,Yes,Not Sure,"The provided information was adequate for assessing whether a dataset can fulfill our intended purpose because it aligned with our criteria for relevance, reliability, and granularity in understanding plastic waste composition and recycling rates.","The provided information was adequate for assessing if data is of sufficient quality because it included details about data collection conditions, data sources, and the reliability of recycling rate metrics.", N/A since the dataset was obtained from public sources without requiring payment.,"In a specific instance, I was unsuccessful in finding datasets that could contribute to a data-driven task related to analyzing the environmental impact of our recycled plastic construction materials compared to traditional building materials. We aimed to quantify the carbon footprint reduction, energy savings, and waste reduction benefits of using our products. The challenges that led to this outcome included a lack of comprehensive data on the life cycle analysis of construction materials in our region and limited access to detailed industry-specific data on environmental metrics.","The new dataset was intended to be obtained from environmental research institutions, government agencies specializing in sustainability, and industry reports on construction material life cycle assessments. However, due to the lack of available data, we were unable to acquire a suitable dataset for our analysis.","We attempted to assess the potential value of the dataset by considering factors such as data completeness, relevance to our research objectives, methodology transparency, and alignment with industry standards for environmental impact assessments.","Source reputation, User reviews, Metadata, Prevalence of missing data, Ability to access full datasets",Yes,Yes,Not Sure,The provided information was inadequate for assessing whether a dataset can fulfill our intended purpose because we could not find datasets with the necessary granularity and specificity in environmental impact metrics for construction materials.,The provided information was inadequate for assessing if data is of sufficient quality because we could not find datasets with detailed life cycle analysis data for construction materials in our region.,N/A since we did not find a dataset that required payment.
"We successfully found a new dataset related to online learning engagement metrics and user feedback. Our goal was to enhance the user experience on Duke Academy by understanding how users interact with our courses and platform features. Our criteria for success included relevance to our business objectives, data quality, and accessibility.","The new dataset was obtained from our internal analytics platform, which collects data on user interactions, course completion rates, feedback surveys, and usage patterns.","We assessed the potential value of the dataset based on its relevance to improving our platform's user experience and the quality of insights it could provide. We also considered the data collection conditions, such as accuracy, completeness, and timeliness.","Source reputation, Metadata, Data collection conditions, Ability to access full datasets, Visualizations",Yes,Yes,No,The dataset adequately fulfilled our intended purpose as it provided valuable insights into user engagement., The data was of sufficient quality based on our assessment criteria.," Since we didn't need to pay for access, the question of whether the data was worth paying for did not apply.","In a particular instance, I was unsuccessful in finding datasets that could contribute to a data-driven task focused on customer sentiment analysis for a new product launch. I aimed to understand customer feedback, preferences, and sentiments to optimize marketing strategies and product features. The challenges that led to this outcome included limited availability of relevant datasets tailored to our specific industry and product niche, as well as data sources with incomplete or outdated information.","The datasets were primarily sourced from public repositories, industry reports, and surveys conducted by third-party organizations. However, the data lacked granularity and specificity required for detailed sentiment analysis.","I assessed the potential value of the datasets based on their relevance to the intended analysis, data freshness, accuracy, and coverage of key metrics related to customer sentiment. However, the datasets fell short in providing comprehensive and up-to-date information.","Source reputation, Metadata, Prevalence of missing data, Ability to access full datasets",No,No,No,The provided information was inadequate in assessing whether the dataset could fulfill the intended purpose due to its lack of specificity and relevance.,"The data was of insufficient quality for detailed sentiment analysis, as it lacked comprehensive and up-to-date information.","Since no payment was required for the datasets, the question of whether the data was worth paying for did not apply."
"We were working on enhancing our AI-powered platform's capabilities to match tenants with their perfect homes more effectively. To achieve this, we needed access to a comprehensive dataset containing detailed information about available rental properties in major cities across Nigeria.","We obtained the new dataset from a reliable real estate data provider that specializes in collecting and aggregating property listings from various sources, including real estate agencies, landlords, and property management companies.","We assessed the potential value of the dataset based on several factors:Source Reputation: The data provider had a strong reputation in the real estate industry for providing accurate and up-to-date information.Metadata: The dataset included metadata such as property types, sizes, locations, amenities, rental prices, and availability status, which were crucial for our matching algorithm.Data Collection Conditions: We ensured that the data collection process adhered to industry standards and legal requirements to maintain data integrity.Ability to Access Full Datasets: We confirmed that the dataset provided comprehensive coverage of rental properties in our target cities without significant gaps or missing data.Sample Data and Visualizations: The data provider offered sample data and visualizations that allowed us to preview the dataset's structure and quality before making a decision.","Source reputation, User reviews, Metadata, Data collection conditions, Sample data, Visualizations",Yes,Yes,Not Sure,The provided information was adequate for assessing whether the dataset could fulfill our intended purpose of enhancing our AI matching capabilities.,The information also helped gauge that the data was of sufficient quality for our data-driven tasks. ," However, additional information on the pricing structure and terms of accessing the dataset would have been beneficial to determine if the data was worth paying for.","Unfortunately, there was an instance where the datasets we found did not contribute effectively to our data-driven task. We were aiming to enhance our AI-powered platform's capabilities in matching tenants with their perfect homes more efficiently. The challenge arose from the datasets lacking detailed information about available rental properties in major cities across Nigeria, which was crucial for our matching algorithm.","We obtained the new dataset from a real estate data provider specializing in collecting and aggregating property listings from various sources, including real estate agencies, landlords, and property management companies.","We assessed the potential value of the dataset by considering several factors, including source reputation, metadata completeness, adherence to data collection standards, data integrity, and the ability to access full datasets without significant gaps or missing data.","Source reputation, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data",Yes,Yes,Yes,The information was adequate for assessing whether a dataset can fulfill our intended purpose.,The information was adequate for assessing if the data is of sufficient quality.,The information was adequate for assessing if the data is worth paying some price to access.
"We were working on developing a predictive model to identify potential growth opportunities for startups based on historical performance data. To enhance the accuracy of our model, we needed access to a dataset containing detailed information about startup funding rounds, market trends, and industry-specific metrics.","We obtained the new dataset from a reputable data provider specializing in startup ecosystem data, including funding data, market analysis, and industry benchmarks.","We assessed the potential value of the dataset based on several factors: the data provider's reputation, the completeness of metadata, the accuracy of data collection methods, and the availability of sample data for evaluation.","Source reputation, Metadata, Data collection conditions, Sample data",Yes,Yes,Yes,"The provided information was adequate in assessing whether the dataset could fulfill our intended purpose. We evaluated the dataset based on its relevance to our predictive modeling needs, such as startup funding rounds, market trends, and industry-specific metrics. The metadata completeness and sample data helped us determine if the dataset aligned with the parameters required for our data-driven tasks."," Assessment of Data Quality:The provided information sufficiently allowed us to gauge whether the data was of usable quality. We considered factors such as the data collection conditions, prevalence of missing data, and the ability to access full datasets. The metadata completeness and sample data provided insights into the data's structure and accuracy, ensuring it met our quality standards for analysis.","Assessment of Data Worth Paying for Access:The provided information helped us assess if the data was worth paying some price to access. Factors such as source reputation, metadata completeness, and the ability to access full datasets influenced our decision. However, additional insights into the data collection methodology and data validation processes would have further justified any potential costs associated with accessing the dataset.","I encountered an instance where the datasets I found could not contribute to a data-driven task effectively. We were aiming to analyze customer sentiment and preferences for our startup growth platform, CChub, to optimize our marketing strategies. However, the datasets we obtained lacked sufficient granularity and real-time updates, hindering our ability to derive actionable insights.",The new datasets were sourced from a third-party data provider specializing in customer behavior analytics.,"We assessed the potential value of the datasets based on their relevance to our analysis goals, data freshness, and coverage of key metrics like customer feedback and engagement patterns.","Metadata, Data collection conditions",No,No,No,The dataset's ability to fulfill our intended purpose due to limited insights into specific metrics and real-time data availability. ,The data's quality as it lacked details on data validation processes and data integrity checks. , The data's worth paying for access as it did not adequately showcase the dataset's value proposition and potential impact on our analysis.
"I encountered a specific instance where I successfully found a new dataset that greatly contributed to a data-driven task. We were working on optimizing our web 3 gaming platform's user engagement and retention strategies. Our goal was to analyze user behavior patterns, preferences, and interactions within the gaming environment to enhance the overall gaming experience and increase user satisfaction and loyalty. The new dataset we found provided detailed gameplay data, including user actions, session durations, in-game purchases, social interactions, and progression milestones.",We obtained the new dataset from a reputable gaming analytics platform that specializes in collecting and analyzing gameplay data from various gaming platforms and genres.,"We assessed the potential value of the dataset based on several factors:Source Reputation: The gaming analytics platform had a strong reputation for providing accurate and comprehensive gameplay data.Metadata: The dataset included detailed metadata such as event types, timestamps, user IDs, and game context, which were essential for our analysis.Data Collection Conditions: We confirmed that the data collection methods used by the platform adhered to industry standards and data privacy regulations.Ability to Access Full Datasets: The platform provided access to full datasets with no significant gaps or missing data.Sample Data and Visualizations: We were able to preview sample data and visualizations provided by the platform, which helped us assess the data's quality and relevance to our objectives.","Source reputation, Metadata, Data collection conditions, Ability to access full datasets, Sample data, Visualizations",Yes,Yes,Yes,"information was adequate for assessing whether the dataset could fulfill our intended purpose of optimizing user engagement and retention strategies. We were able to review the dataset's content, metadata, and relevance to our objectives, which helped us determine its potential usefulness."," If the data was worth paying for, given its value and potential impact on our strategies and outcomes: The provided information allowed us to assess the value of the data and its potential impact on our strategies and outcomes. Considering the dataset's relevance to optimizing user engagement and retention, we believed that it was worth paying for, as it could significantly impact our platform's success and user satisfaction.","If the data was worth paying for, given its value and potential impact on our strategies and outcomes: The provided information allowed us to assess the value of the data and its potential impact on our strategies and outcomes. Considering the dataset's relevance to optimizing user engagement and retention, we believed that it was worth paying for, as it could significantly impact our platform's success and user satisfaction.","In a specific instance, I encountered a dataset that could not contribute effectively to a data-driven task. We were attempting to analyze user behavior patterns to optimize our platform's user experience and engagement strategies. The challenges that led to this outcome included:
Limited data granularity: The dataset lacked detailed user interaction data, making it challenging to derive meaningful insights.
Data format issues: The dataset was in an incompatible format with our analysis tools, leading to difficulties in processing and extracting relevant information.
Insufficient data coverage: The dataset did not cover a wide range of user demographics or usage scenarios, limiting its applicability to our objectives.",The new dataset was obtained from a data aggregator specializing in user behavior analytics for web platforms.,"We assessed the potential value of the dataset based on various factors, including its relevance to our objectives, data quality, coverage, and usability.","User reviews, Metadata, Data collection conditions, Sample data",Not Sure,No,Not Sure,The information was inadequate to gauge if the data is of sufficient quality because of data format issues. , The information was inadequate to determine if the data was worth paying for due to insufficient data coverage.,The information was inadequate to assess whether the dataset could fulfill our intended purpose due to limited data granularity. 
"One instance where I successfully found a new dataset that contributed to a data-driven task was when we were analyzing customer feedback and product performance for our new line of eco-friendly paints. We wanted to understand customer preferences, satisfaction levels, and any issues they encountered with the new paints to improve our offerings and marketing strategies.","We obtained the new dataset from our customer feedback platform, which aggregates feedback from various sources such as surveys, reviews, and direct communications with customers.","To assess the potential value of the dataset, we considered several factors:
Source Reputation: The feedback platform has a strong reputation for providing reliable and comprehensive customer feedback data.
Metadata: The dataset included metadata such as customer demographics, paint usage patterns, ratings, and comments, which helped us contextualize the feedback.
Data Collection Conditions: We ensured that the feedback collection process followed best practices to maintain data accuracy and integrity.
Prevalence of Missing Data: We reviewed the dataset for any significant gaps or missing data points that could impact our analysis.
Ability to Access Full Datasets: We confirmed that we had access to the entire dataset without restrictions.
Sample Data: The platform provided sample data that allowed us to assess the dataset's structure and relevance to our objectives.","Source reputation, Metadata, Data collection conditions, Ability to access full datasets, Sample data",Yes,Yes,Not Sure,The provided information was adequate for assessing whether the dataset can fulfill our intended purpose. It contained relevant customer feedback and usage data that aligned with our objectives for analyzing product performance and customer satisfaction.,"The information was sufficient for gauging the data's usability, as it included structured data points and qualitative feedback from customers, allowing us to perform thorough analysis and derive actionable insights.","Regarding whether the data is worth paying some price to access, the provided information was sufficient in demonstrating the dataset's value in improving our products and marketing strategies. It would have been justifiable to pay for access considering the depth and relevance of the data to our business goals.",I was unsuccessful in finding datasets that could contribute to a data-driven task when I was trying to analyze customer preferences for new product features. The challenge was the lack of comprehensive customer data with relevant attributes.,I obtained the datasets from internal databases and external market research sources.,I assessed the potential value of the datasets by considering their coverage of essential customer attributes and their alignment with our research goals,"Metadata, Data collection conditions, Sample data, Visualizations",No,No,No,The dataset was inadequate for assessing whether it could fulfill our intended purpose due to missing key customer attributes essential for analysis and decision-making. ,The data did not meet the standards of sufficient quality required for our analysis and strategy development. ,"Given the deficiencies in data quality and relevance, it would not have been justifiable to pay a price to access this dataset as it did not align with our business needs and goals."
"The most recent example is a simple one, but it's the data that I found from the USA National Phenology Network of spring arrival throughout the country, which is judged based on the blooming of certain species of flower. My goal was to regionalize it for the region that I work in, and show the data that would most concern our local residents. The questions we were looking to answer were, ""When did spring arrive for my area this year?"" and ""How does this year compare to previous years?"" and ""How does my area compare to the rest of the country?"" Using the GeoTiff provided by the network, we were able to create our own maps that answered those questions. ",The USA National Phenology Network's data request website,There were some visualization examples on the landing page that gave an idea of what to expect from the data. But there was no way to truly know until I downloaded it and started playing around with the data in QGIS. ,"Source reputation, Metadata, Ability to access full datasets, Visualizations",Yes,No,Didn't need to pay,"Because our end result was the same or very similar to the example visualizations they showed on their landing page, I knew we could at least achieve something similar. ",There was no way to truly analyze the quality of the data until I downloaded it since there was no user-facing data explorer.,Didn't need to pay.,"There was a time in a job I had a couple years ago that involved getting traffic data from a GPS company that was going to show traffic patterns on a particularly busy day in my city. The data did not include certain aspects of the data that the company was unwilling to share, but would have been of interest to the readers. ",From contacting the company itself.,"We didn't end up paying for it, but we didn't know until we received the data from their communications office. ",Source reputation,No,No,Didn't need to pay,We relied heavily on the reputation of the company as a good source for traffic data concerning our story.,"We didn't know for sure until we received the data, whether or not they could answer all our data questions. it turned out they could not. ",We didn't have to pay.
"Found a pathways data (pathway.txt) within a GTFS package from Sydney's Transport for New South Wales page after signing up for developer access. I needed it to get station info of many stations in Sydney, for example the transfer times from platform to platform, exit locations, station locations, and accessibility information (i.e., is it accessible via stairs, elevator, escalators, slope). No particular criteria for success as there are already existing guidelines on what the pathways.txt's mandatory and optional fields are.",https://opendata.transport.nsw.gov.au/user/login,"How extensive the fields are, and how granular the data is. For example the transfer times are not just estimates of 5 minutes, but are very specific and diverse i.e., 13 mins, 7 mins.","Source reputation, Metadata, Ability to access full datasets",Yes,Yes,Didn't need to pay,"i knew exactly that i needed this specific format of files (pathways data) to get a json output of each station, so there wasn't any ambiguity. You either have it or not.",That it includes the mandatory fields as well as many optional fields and files for extra comprehensiveness.,"i think given that it's open data that the government has obligations to share as a public service, i will also assume that i do not need to pay for access.","It was a an digital elevation model (DEM) from ALOS PALSAR that I thought we could use to improve the elevation layer in our routing algorithm, so that we're making better recommendations for walk paths (i.e., not overestimating where hilly or slope parts are and not recommending people to walk the route). However it did not work out as the data was too large and it'd take too much time for us to simplify it for our use.",ALOS PALSAR - https://www.eorc.jaxa.jp/ALOS/en/index_e.htm,"the data was under NASA EarthData, so having not much domain knowledge in elevation layers I implicitly trusted the authoritative source.","Source reputation, User reviews",Not Sure,Not Sure,No,"the data had a lot of parameters, so I wasn't sure how it might fit into our routing algorithm as this would be the purview of the routing engineer team.","The data had lots of jargon, so I wasn't able to really assess the quality.","As mentioned I think the issue lies with my lack of domain knowledge in it, so I wasn't sure if it'd have been useful for our algorithm and hence I could not ascertain if it was worth paying for it."
"When building a model for a looking at potential conspiracy theories, I found a dataset from researchers at Pennsylvania State University that brought together posts that had been fact-checked. I needed to find the actual posts themselves, rather than just third-party fact checks of the information. The goal was to use this dataset as part of a test set for model performance, by having human labellers label the type of misinformation in each post.",Github,"I reviewed several lines of the data and found that the claims were largely similar and largely were clear cases of misinformation. This clarity, and the clear methodology laid out, helped me be confident that my human labellers could make distinctions on the data.","Source reputation, Data collection conditions, Ability to access full datasets, Sample data",Yes,Yes,Didn't need to pay,It is very helpful to have the above mentioned datapoints when assessing data and whether it can fulfill my purpose,"The sample data is good for checking data quality, though not perfect","Minimally useful, as the price is often a business decision rather than a data decision","For the same project as previously mentioned, I explored other datasets in the misinformation space. Multiple instances saw datasets that were classified as ""misinformation claim"" data when in fact they were only a list of third-party fact checks easily pulled from a fact-checking website",Github,I explained the sample data as well as the little methodology and metadata that existed.,"Metadata, Data collection conditions, Ability to access full datasets, Sample data",Yes,Yes,Didn't need to pay,Adequate as I could make a quick decision after seeing the sample data,Adequate as I could make a quick decision after seeing the sample data,Adequate as the data did not fit my conditions so I would not pay for it
"I was working on a project about how lack of access to child care hinders working parents, particularly women, from reaching their full economic potential. This project was part of a collaboration between the AP and many different newspapers across the country, involving dozens of different journalists. My editor asked me to find data that could be relevant for the series. I looked through many different sources and considered different datasets, and in the end decided to analyze IPUMS CPS data on maternal employment rates plus data from the Census Household Pulse on child care disruptions. My criteria for success is that I wanted datasets that would give me interesting and novel insights that would make for a compelling story. My co-author on the article specifically said that she wanted some kind of big number or attention-grabbing finding that would stand out in the headline or the introduction of the story. At the same time, I wanted the datasets to be from a credible source, not just some random market research survey. Also, I wanted data that could be easily localized. Since this was a large collaboration, I was tasked with writing a guide for journalists about they could use data about their own particular city/state to write a local story. Here is the story that was ultimately published: https://apnews.com/article/daycare-child-care-college-degree-moms-ac72f1227844eae0281305835e07273b","https://cps.ipums.org/, https://www.census.gov/programs-surveys/household-pulse-survey/data.html","Since both datasets come from the Census, I consider them to be dependable sources, and they are both available for free. As a journalist, I rely largely on public data sets. The only time I ever pay for data is if I have to pay a fee for a public records request, but even then it's rare I have to pay anything. I sought out IPUMS specifically because I saw a Washington Post article about women's employment rates which mentioned a study by the Penn Wharton Budget Model. I looked up the original study (https://budgetmodel.wharton.upenn.edu/issues/2023/11/27/explaining-prime-age-womens-employment) and saw that they used IPUMS data, so I decided to do the same. That way I was able to check my calculations against theirs, and since I got the same results, I knew that my methodology was sound. (The difference between their analysis and mine is that I took it further, also looking at employment rates by state and metropolitan level and race.) For the Household Pulse, I had worked with that data for earlier stories, and I wanted a way to connect the finding on employment with findings on child care disruptions. ","Source reputation, Extensive documentation, help from my colleagues, inclusion of standard errors, use by other researchers/journalists, the fact that it is a government dataset",Yes,Yes,Didn't need to pay,The documentation allowed me to see what kind of variables I could access,"Use by other researchers + documentation I found online + standard errors (where they existed) + help from my colleagues helped me understand how to investigate whether my findings were credible (in cases where standard errors were not published, I was able to find ways to calculate them myself). ",On principle I believe that government data should be available free of charge. ,"I was asked to see if there was data on grants or contracts U.S. colleges received from Israel. Colleges are required to report all foreign gifts. However, a colleague told me he had already done previous reporting showing that there is massive underreporting in this area. A very quick scan of the data available showed that it was almost comically unhelpful, obviously inadequate, and would barely tell me anything. ",the data appears on https://sites.ed.gov/foreigngifts/,"Conversations with a colleague, also looking up gifts from Israel and seeing the information that was available. It was obvious quite quickly that the information was nearly useless because it said so little. For example, the description of one grant was just a single word, ""research."" ","Source reputation, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Advice from colleagues",Yes,Yes,Didn't need to pay,The full dataset was available.,It obviously wasn't. ,I would not pay five cents for this data. 
"I FOIA-d all fifty states, and a few cities, for data on their rape kit backlog for my data investigation. I was trying to give my audience a sense of how many rape kits were left untested in crime labs and law enforcement agencies across the country, in 2022. My criteria for success was getting data from as many states as possible -- because then local news outlets would be able to report on the problem -- and to be able to visualize and explain the data (its limitations are a big part of this), the implications of a rape kit backlog, etc. in a way that is compelling to an audience that might not even know what a rape kit is.

You can read my work here: https://usafacts.org/articles/how-many-rape-kits-are-awaiting-testing-in-the-us-see-the-data-by-state/","I FOIA-d crime labs, state attorney general's offices, and law enforcement agencies.","If, from the pdfs and spreadsheets I was sent, or the reports I found online, I could determine the number of rape kits that were left untested in law enforcement agencies and crime labs in each state at the end of 2022, I used the data. Also under consideration was how well I could explain the limitations of the data (ex: this number excludes these jurisdictions, data from these years, etc.) ","Data collection conditions, Prevalence of missing data, Ability to access full datasets, Ability to speak to someone involved in the data collection process",Yes,Yes,No,"My answers for the next three questions all kind of merged into one answer, so here it is: 

The data on the national rape kit backlog was already so bad to begin with that I did feel that whatever data I was able to obtain would be of value. Of course, my final investigation is riddled with footnotes -- few of the datasets I obtained through FOIA included data for all of the years I was looking for, for all of the crime labs/law enforcement agencies in the state, etc. So the standard of the quality of the data was low, but I used as much of the data as I obtained because I felt like I could sufficiently explain the limitations of each dataset. 

Data collection conditions -- knowing whether a crime lab had their license taken away during years of testing due to fraud, or whether a state changed its laws so the data reporting requirements changed, (both situations I encountered) etc. -- very useful for knowing whether the dataset can fulfill my intended purpose. Often knowing the data collection conditions hinged on whether I was able to locate/speak to someone in a lab or state agency who was willing to take the time to explain to me when these situations applied.

Prevalence of missing data -- not prevalence, but extent. Knowing which years or for which counties/cities, etc. data was excluded was very useful.

Ability to access full datasets -- again this was almost never the case in this area (as is the case with crime data in general) so it's not something I expected of any state or law enforcement agency. ",See above. ,See above. ,"A lot of the time I find that the findings from my data analysis are not interesting enough to include in an article. My data analysis being published hinges on me also being able to find something interesting and visualize it in a way that is compelling to an audience. This happened when I was trying to use data to portray how pharmacies are changing in the state of Ohio (CVS has been shuttering a lot of pharmacies and I wanted to see if that was true for local/independent pharmacies as well), but there were no noticeable trends in commercial or local pharmacies opening/closing that I could make a visualization out of. ",Ohio Board of Pharmacy,"Didn't need to pay! I just played around with it and made a number of different bar charts -- broken down by type of pharmacy, area, etc. to see if I could make an interesting viz. In the end I couldn't. ",Visualizations,Yes,Yes,Didn't need to pay,"Again just putting my thoughts here in one response.

I was confident about the quality of the data because (a) the Ohio Board of Pharmacy got back to me with the data I asked for within a day, and they were very responsive to my questions. Ultimately, I think visualizations were kind of the only way I could think of making the data interesting to a local audience. ",See above.,See above.
"I was tasked with an analysis of the impact of the fields of study on the tendency of immigration for foreign-born students in the US. As the topic mainly concerns principal component analysis, I was looking for immigration records that are (1) sizable and (2) comprehensive. The number of records induces a higher confidence value and the number of fields within one record confirms the principality of the component.",I obtained the dataset through a Freedom of Information Act (FOIA) request to Immigration and Customs Enforcement (ICE).,The dataset is valuable since it comes directly from a government agency containing all the requested fields and the ICE has a reputation of data maintenance.,"Source reputation, Data collection conditions, Ability to access full datasets",Yes,Not Sure,Didn't need to pay,"Yes, the dataset is produced upon request, so while making the request, we had to plan for how the dataset can be used for our purposes.","Not sure, even though we knew that the ICE has good reputation in data collection, before seeing the dataset, it is hard to gauge the extent to which the ICE is willing to disclose the data.",We did not need to pay.,"I could not recall any specific instance, but in general I found when working in economic research, public datasets are usually fragmented --- it is not uncommon to see that the records from each year use different metrics, which makes analyses of trends difficult.",N/A,"We assessed the value by looking at a small sample from each year, primarily looking for whether the collected data are coherent.","Source reputation, Prevalence of missing data, Sample data",Yes,Yes,Yes,"Yes, from a sample, we can usually know what fields it records and in what metrics.","Yes, same as the above.","Yes, same as the above."
"I was trying to find a dataset which could be used as a demonstration for multiverse analysis, as I wanted to use the dataset to implement a multiverse analysis and visualise the results using a visualisation tool that I was developing. I primarily looked at prior work which implemented similar analysis, and settled on one which simulated the data. I used their data-generating process to simulate my own data, however.",From the supplementary materials of a previously published research article,The research paper had demonstrated the value in the manuscript,"Metadata, Description of the data-generating process",Yes,Yes,Didn't need to pay,"Because I control the data-generating process, it was essentially guaranteed","Because I control the data-generating process, it was essentially guaranteed",I didn't pay for the data,"For a study on how to visualise datasets with missing values, I tried to use datasets from Our World in Data to create (amputed) datasets with missing values. However, these amputed datasets didn't let us investigate the effects we wanted to study as we couldn't guarantee the data met certain criteria that was required for this study.",Our World in Data,Primarily based on metadata and topic,"Source reputation, Metadata, Ability to access full datasets",No,Not Sure,Didn't need to pay,"Because I intended to perform further manipulations on the data, the provided information was not sufficient for what I was looking for. However, the metadata was helpful to understand whether I want to even perform the initial transformations for this study.","I don't recall, but sometimes the meta data would not be complete or 100% accurate, which made it challenging to assess data quality. For instance, the dataset would have missing values but this information was not summarised in the metadata.",I didn't pay for the data
"Okay. I often collect my own dataset. But when I was modeling election forecasts and analyzing user experimental data, I did search for U.S. census and economic data. ","I don't quite remember where I got the economic data. But I got the US 2020 census from https://www.census.gov/, and eventually, I went with an R package https://walker-data.com/tidycensus/ because it is much easier to integrate them with my R code","I will likely download any seemly relevant dataset if it is free and fit my disk lol. For the census data, it's quite large and slow to download. I obtained the part I need for my research. If the dataset is not free, I will likely download it if it is under 100 USD. Beyond this point, I will need to carefully assess if I really need this dataset for my work, or how much my time it could save, and discuss it with my mentor.  ","Source reputation, Sample data",Yes,Yes,Didn't need to pay,"I got the R document and first downloaded a small portion, and realized they were adequate. ","I trust US census. The R package provides API to access and manipulate it. The sample data provides columns I needed, and the R document is exhaustive. ","Didn't need to pay, but it took some time (20m?) to download. I estimated based on the time of what I had. ","okay. Today I was searching for a live cost dataset for personal decisions, not for my research. I realize these datasets are often sparse, and sometimes don't provide data about the city I need. Also, I don't know which website I should trust. ","Google search. Like https://www.forbes.com/advisor/mortgages/real-estate/cost-of-living-calculator/, https://www.numbeo.com/cost-of-living/, and https://smartasset.com/mortgage/cost-of-living-calculator","hmmm, this might sound a little odd. I realize when a website provides an overview for live expense, I want details to entrust it. However, when a website provides details (different dimensions), I'm annoyed and want an overview for a quick comparison. So maybe my criterion is utilities? what a fuzzy word... ","Source reputation, Sample data",No,No,Didn't need to pay,"Sometimes the dataset is too detailed, and I have to do mental aggregation. Sometimes, it is missing the city I need. Sometimes, it provides two numbers, and I don't know whether I should trust it... ","I guess forbes is well-known and I could trust it a bit more, but I really don't know trustworthy websites in this domain. I feel I just don't have enough knowledge to assess if data is of sufficient quality? I don't know what information I need to assess the quality? 
",Didn't need to pay
I used datasets as supporting evidence to a video installation about waste management in Romania,Eurostat and national dataset,Based on the project's budget,"Source reputation, Metadata, Data collection conditions, Ability to access full datasets",Yes,Yes,Yes,Has data that are in line with my assumptions,has enough variations in categories and values,"reputable sources, reliable data collection, constant updates","It happens often that a dataset doe not sustain my argument: most of the time it's due to the specific topics or data items listed in the dataset, or simply because it is a good dataset but for slightly different type of research","Same source, different dataset",I use free data to make visualizations and check with the client/team whether it's worth paying for full access,"Source reputation, User reviews, Metadata, Data collection conditions, Ability to access full datasets, Sample data",Yes,Yes,Yes,data are in line with my assumptions,data is organised with geographical scale and time ,"reputable source, other people have already download it"
"Recently, I was looking up something about a US Census Data resource and I quite accidentally stumbled across a dataset that we were also looking for. The dataset that we were looking for, although I'm not sure if the search had really begun was data about the number of Citizen Age Voters of different races at different geographic levels (e.g., county, tract, etc.) across time. I'm not sure if our team had not conducted a thorough search initially, but these were data we needed access too and I had been told they weren't available or easily accessible. 

The data driven task that we were trying to achieve was to show disparities in polling places accessibility in terms of the number of voters each polling location, or aggregation of polling locations served. Race indicators are important to our analysis because we were interested if polling accessibility differed by race. For example, in predominantly Black counties does access to polling locations differ substantially?

The Census Bureau ended up having the data I was looking for, but I was looking for Number of people of voting age and who are citizens for each county during each election year and then broken down by race.",The United States Census Bureau,"I downloaded the data file, the one I thought could be relevant and opened it. I think looked to see which columns and years existed in the data. With this data this assessment is fairly simple, with other data this can be a fairly complex task. 

For example, we also work with polling location data, and these data are messy. There are duplicate locations, there are missing values where we need there to be values. Sometimes the aggregate number of rows doesn't make sense year-over-year.","Source reputation, Metadata, Ability to access full datasets",No,No,No,Normally in provided information there is a summary about the data. You're lucky if you get a data dictionary to go along with that. But it's unlikely that someone has tabulated the level of missingness for all the columns which is often essential for having a robust analysis. ,"Normally inadequate. It can be challenging to anticipate data quality issues before you actually start working with the data. Because others may have very different intended use cases, a dataset with quality issues may still serve their purpose and not yours. It's often ver y case specific.","I would pay for a resource if I had access to the sample data and could actually use and look and look at the sample. Additionally, if similar organizations had recommended the data or were using it for a similar means. ","We then asked a similar question to the scenario outlined in the last Positive Experience question, but this time by age. The Census Bureau has some age-specific data, but because it is aggregated into the Census Bureau's age bins (e.g., 10 - 17, 18 - 25) these data were less accessible to our use. The age aggregations are limiting in some cases because citizens are allowed to start voting at the age of 16, so 16 and 17 year-olds are omitted from the analysis because of how the data are structured. The age bin below contains a large number of children who would not be of voting age. So because of the way the Census has designed their aggregations, it makes it challenging to do voting age analysis for youth voters.

Another data source that we were looking for was voting turnout. Turnout that is broken down by county and census tract, by race, and by age. ",The Census Bureau,"I assessed that the dataset from the Census Bureau was still useful and relevant, but we would have to caveat that it excluded certain ages. ","Source reputation, Data collection conditions, Ability to access full datasets",No,No,No,"I don't remember for certain, but I doubt they included the specific age breakdown categories in the description of the data.",Similar answer to the one already given.,Similar answer to the one already given.
"I only work with internal data sources for work projects, however I've searched datasets and notebooks online for ideas to enhance my data analysis. Specifically, I remember searching through multiple online notebooks working with API data of wearables to get ideas for feature engineering/selection while modeling. ",Github and Kaggle,I was only looking for notebooks working with data sourced from the wearable device's API to help me ideate data processing methods.,"Source reputation, User reviews, Data collection conditions",Not Sure,Not Sure,Didn't need to pay,"Cannot answer - wasn't using the dataset itself, rather trying to understand collection/analysis procedures.",Correctly extracted data from the developer API is useful but not completely validated.,Not in my case as I wasn't actually using the online datasets for my own analysis.,"I had an offseason project that dealt with the tracking the changes in headcount and compensation of regional athletic department employees. This data was not publicly available in a reputed central source, not regularly updated, and did not list exact employee unit and status information. ",Through individually contacting regional university HR departments and live web-scraping of official public directories.,The data needed to be directly sourced from the institution's athletics department.,Source reputation,No,Not Sure,Didn't need to pay,Only a data pipeline directly connected to the official university employee database sources with detailed departmental information was adequate for the intended purpose. Datasets available online were not live and thus inadequate.,To validate the source there needs to be a direct connection to official institutional directory data.,Do not need to pay for the data.
"For my Internet measurement research, I was looking for IP-geolocation dataset and I found a dataset, called MaxMind, which I eventually used. MaxMind provides a free version as well as a paid version of the data. Free version is less precise and less complete than the paid version.

Trying to achieve: I was trying to use this dataset for my research. IP-geolocation is crucial for projects that involve measuring reliability of the Internet at various geolocation and different geographical granularities like country-level, state, city and county levels.

Criteria for success: whether the dataset provider has the description about the following points

(a) documentation about the schema of the dataset

(b) how easy it is to use it - if I can download some version of it easily (and without paying) and if there are scripts/libraries for parsing and using the dataset

- After downloading, I also want to parse it/play with it quickly and have some output. While doing this, I also keep an eye on how popular this dataset and its parsing tools, are on the Internet - I google the errors that I get while playing around and check how many people have interacted with the error/similar errors in forums like stack overflow, etc.
(c) when was it last updated (IP-geolocation data is time-sensitive and it changes with time)

(d) notes about the completeness/accuracy of the dataset - for e.g. comparison of accuracy of free vs. paid version

(e) If it is under any license that would allow me to share the results produced by using this dataset",I downloaded from the official website of the data provider (maxmind - https://dev.maxmind.com/geoip/geolite2-free-geolocation-data#ip-geolocation-accuracy),"I followed the following criteria to assess the potential value of the dataset:

(a) the reputation of this dataset in the research community and whether any previously published papers have used it

(b) if by reading the description of the schema, I could see the presence of data that I need

(c) how rigorous are the documentations provided by the data provider

(d) looking at how well-maintained and complete their website looks

(e) if there is a license","Source reputation, User reviews, Metadata, Ability to access full datasets, My use case - I did not had to have very precise geolocation accuracy for my project.",Yes,No,Yes,"Here is how each of the provided information influenced my decision:
Metadata - 
- provides last updated,
- data in csv format (easier to parse)

Source reputation- 
- good enough (used in papers like - On the Accuracy of Country-Level IP Geolocation)

User reviews - mostly by word of mouth from people in my research group/collaborators who have used this data in the past, hinted that this would be good enough for my use case.

Ability to access full datasets - I can easily download the dataset. One of the formats that this dataset was available in, is csv, which is easier to parse and use.

(Other) My use case - I did not had to have very precise geolocation accuracy for my project.

Sample data - is not available and also I think in this particular case, sample data would not help much either, i think.

Visualizations - there wasn't any visualization.","The data collection methodology is not disclosed - this limited my ability to judge if the quality is sufficient.

Also in general, it is difficult to evaluate the quality of IP-geolocation datasets.","source reputation - from published papers as well as from user reviews (my research group and collaborators) - which has some results about the accuracy of the dataset, i knew it would be sufficient for my use-case.

there is also a reputation (as well as user review) of another “better” IP-geolocation dataset in the community, namely NetAcuity, which is a paid dataset (i.e. there is no free version of this dataset).

NetAcuity discloses their data collection methodology, and they also claim to be more accurate, which makes me more confident to purchase their paid version of data, in case I would need the paid version.

My use case - importantly, for my use case, I did not need the most precise IP geolocation data.","This was an instance during another research (different from the one that I described in the positive experience), where the goal was to evaluate the quality (for e.g. how realistic are these datasets) of some well-known and commonly-used datasets for investigation and detection of Advanced and Persistent Threats (APT) attacks.

My unsuccessful experience was with one of the datasets - DARPA TC5 (transparent computing 5), which is open-sourced by DARPA.

Trying to achieve: I was trying to download the commonly used datasets in order to analyze them.

Challenges:

- Lack of good documentation on how to parse the data - the documentation available does helps somewhat but does not help me completely parse the dataset.
- Not well maintained - the documentation about the data is old and outdated
- Tools to visualize this dataset don’t work as intended by the authors - looks outdated and also the tools lack documentation as well
- very hard to find any help - it is not clear who were the authors of this data and it was not possible to reach out to them for any help
- recent github issues are not addressed by anyone
- Other researchers who have used in their works are also hard to reach out to/not super helpful","it is hosted by another organization Five Directions Inc. on their Google drive, so downloaded from there.","(a) Reputation and it’s use in research community - some recent papers talk about using this dataset in their work for detection of APT attacks.

(b) Freely available - this is one of the few datasets for APT detection that is freely available for research. Usually other similar datasets are owned by companies and are private.

(c) Metadata: I checked the number of files and data file sizes in google drive - this dataset seems more comprehensive that other similar datasets - there are more number of files in this dataset and file sizes are larger compared to other similar datasets. The schema of the data is also available.","Source reputation, User reviews, Metadata, Data collection conditions, Ability to access full datasets",Yes,No,Didn't need to pay,"- source reputation - DARPA is a government organization with a long history of supporting research and in this case, they mention that their aim is to stimulate research in APT investigation and detection.
- user reviews - some research papers mention the use of this dataset
- metadata - the data source provides lots of metadata and tools (but unfortunately they are outdated)
- data collection conditions - the data provides describe their process of creating this dataset
- ability to access full datasets - full dataset is available for downloading for free","It is very hard to tell - in fact, this was my research problem: ""evaluation of quality of datasets used for APT detection"". Having said that, just based on the description of the dataset, it looks like it might be okay. ",This data is made open source by their providers and is freely available to everyone.
"During my last internship, I aimed to enhance a real-time NPV valuation model for car loans by integrating more comprehensive data. Specifically, I sought additional datasets on customer credit histories, vehicle specifics, deal structures, and macroeconomic factors.

To improve model accuracy and robustness by incorporating diverse and relevant data sources. Here are some criteria for success:

1. Data Completeness: Must cover all required variables.
2. Data Quality: Minimal missing values and inaccuracies.
3. Relevance: Pertinent to car loan performance.

Outcome:
The integration of these new datasets significantly improved the model's performance, enhancing its accuracy and computational efficiency, and optimizing the underwriting process.
",From the company database,I will evaluate the datasets based on my previous working experience and how those features are important to our model building process.,"Source reputation, Metadata, Prevalence of missing data, Ability to access full datasets",Yes,Not Sure,Didn't need to pay,"Metadata , Prevalence of missing value, ",source reputation,"source reputation, Metadata , Prevalence of missing value, ability to gain the full datasets","During a project to evaluate the influence of nearby competitor stores on daily sales predictions and strategic store location decisions, I aimed to improve the existing sales formula by incorporating distance as a significant factor.

Objective:
To develop an iterative algorithm that correlates distance ranges with sales forecast coefficients in a multi-competitor environment, hypothesizing that larger distances would yield larger coefficients.

Challenges: 
Front-line Data Collection Delays: Front-line staff spent too much time measuring data and often did not follow instructions properly.

To mitigate this, I proposed using Google Maps walking data for initial distance estimates while waiting for the front-line team’s data.

Outcome:
Despite the efforts to use alternative data sources, the project faced delays, and the initial algorithm did not perform as expected due to the inaccuracies 
",Google Map,Data Accuracy,"Source reputation, Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets",Yes,Yes,Yes,"MetaData, Prevalence of Missing values",Source Reputation,"MetaData, Ability to access the full dataset"
"I was studying the socioeconomic impact of a fare-free public transit program in Kansas City, MO, for my research thesis. I was looking for a dataset that ideally contains: 1. Information on whether the respondent resides in Kansas City during a specific time period; 2. information on the respondent's employment status, commuting method, and other demographic info (age, gender, race, income, education, etc.); and 3. The specific routes and geographic areas covered by the fare-free public transit program. 
Number 3 was difficult to achieve, so I primarily focused on obtaining a dataset that fulfilled criteria 1 and 2. The Current Population Survey (CPS) contained most of the information I needed. However, it did not include data on whether a respondent resides in Kansas City, MO, or the counties that constitute Kansas City, MO, due to confidentiality reasons. Therefore, I ended up compromising by using data on the state level (residents in Missouri). ",IPUMS-CPS,The most important factor is whether the dataset contains the data I needed. ,"Source reputation, Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data",Yes,Yes,No,"Prevalence of missing data, ability to access full datasets, metadata, and sample data are most helpful in terms of helping me determine whether a dataset is suitable for my research. This information allows me to understand the completeness, accessibility, context, and structure of the data, ensuring it aligns with my research needs.","The datasets I have been using mostly come from reputable government or academic databases, so data quality hasn't been a concern for me. ","If I needed to pay to access the dataset, it is important to know how much I would have to pay to make a decision, considering my budget. Sample data is also a MUST-have because paid datasets are often not accessible for preview. I will need to at least have a glimpse of of the dataset to understand what data are available, the level at which data are they collected (individual vs. aggregate), and the level at which they are representative (county, state, etc.). ","In my research proposal on the impact of work-from-home on individual happiness, I was looking for a dataset that contained: 1. whether an individual engaged in remote or hybrid working during a certain time period; 2. the individual's rating of their happiness (e.g., very happy, quite happy, not happy), time spent or frequency for leisure activities, etc. Unfortunately, the most relevant dataset I found did not contain info on whether the individual worked remotely or not (and many other databases do not include this info, either). I ended up terminating that project due to the lack of necessary data. ",American Time Use Survey (ATUS),Same as previously answered.,"Source reputation, Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data",Yes,Yes,No,Same as previously answered.,Same as previously answered.,Same as previously answered.
"In a geo-spatial relevant research, to find a convincible dataset is challenging. According to the successful experience of achieving this goal, the exploration in corresponding thesis as well as using sufficient resources online are always crucial. Try to figure out if the reference section in a previous article mentioned any useful hints in their dataset.",Links / platforms where previous researches mentioned,Based on if the dataset is widely used in previous analysis. Also according to it’s collecting methods.,"Source reputation, User reviews, Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data, Visualizations",Yes,Not Sure,Not Sure,Need to perform data exploration first. ,Based on the dataset description and how the data was collected,If I have money… if who pay for it says yes,"It was difficult to find out a trajectory dataset regarding geo-spatial research in gas pipelines, where I failed to figure out a desirable data source. The reason might lie in the restrictions and compliance in the data.",Online open data source,If it's for free I will download it to see if work. Usually free sources are enough,"Source reputation, User reviews, Metadata, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data, Visualizations",Yes,Yes,Not Sure,Need to download and test it’s subset,Based on data source description and others comment,Based on advice from fundings…
"I was trying to model methamphetamine use behavior in the U.S. I didn't have any hard criteria for my search given the limited availability of such datasets, but the goal was to gather information to around the population who use methamphetamine, demographics, physical and mental health characteristics, methamphetamine use pattern, utilization of the healthcare system etc. In this case, I was searching not only for datasets but also among literatures which might have reported relevant pieces of data. ","From U.S. Substance Abuse and Mental Health Services Administration (SAMHSA), the dataset is from the National Survey on Drug Use and Health (NSDUH) https://www.samhsa.gov/data/data-we-collect/nsduh-national-survey-drug-use-and-health","The website provides an online data analysis tool for researchers to perform simple one-way tables and two-way crosstabs on public use data files. For each variable, the tool provides a snapshot of the levels. Also, it provides a code book/documentation for a more detailed explanation on the variables and how it's collected/computed (the question asked in the original survey/whether something is imputed). Finally, typically, this type of dataset comes with a brief summary/report with the key findings and some figures/tables. ","Source reputation, Data collection conditions, Prevalence of missing data, Sample data",Yes,No,Didn't need to pay,"Yes, most importantly because my expectation was low. The quality of the data is well-established, and it's widely-used in the literatures (so I guess this is some form of user review although I didn't select it in Q4). From the online tools and documentations, I had a pretty good idea about what the dataset contained. ","I'm not sure what ""usable quality"" is referring to in Q6, but I think this is harder because sometimes we ask more from a dataset if the goal is to use it in analysis. For example, for my purpose, missingness and data collection conditions are important but not necessary given that the average quality of data on this topic (methamphetamine use) is somewhat poor. However, if I were to incorporate this dataset in some models or formal reportable analysis, then missingness and collection do matter. ","I didn't need to pay, but if I were, I believe the decision would largely depend on funding and alternatives. If the dataset is reasonably priced and there is no close alternatives, then I would consider the following: Certainly, the data providers cannot publish the whole dataset, but I would assume that some sort of sample data along with documentations should be provided. User review is also important for me because I would like to know what other people are doing with this dataset (ideally like on Kaggle there are datasets and notebooks that conduct analyses on these datasets). Other measures including the data collection process and missingness are also important for determining the quality. ","I was trying to find some data to demo the application of experiment design methodologies in healthcare for my causal inference research group. I was doing this because I wasn't ready to use the real medical data gathered by our . I was able to get some online datasets. There wasn't a specific topic that I was looking for, which was probably why I failed, but I was looking for some structure in the dataset to support the analysis for doing experiment design. ",Kaggle and some other online datasets platform,"I look for data collection conditions, missing data, and sample size. I also look for definition of the variables to try to get the relationship between variables. ","Source reputation, User reviews, Data collection conditions, Prevalence of missing data",Yes,Not Sure,Didn't need to pay,"For this project, I only got to know what I needed from the dataset after browsing through a couple of them. The criteria that I picked out from the list in Q4 are reflections of my initial search. I paid more attention to the structure of the dataset once I realized that's very important for my purpose. ",I think it'll be even more difficult to know since sometimes I realized the dataset wasn't good enough after having done some analysis on it. ,"Not sure, didn't need to pay, would be a similar thought process as the previous. "
"I once wanted to find primary election data from State government website. I wanted to analyze the primary election results throughout the recent decades. I defined ""success"" in finding datasets to satisfy the following criteria: 1) the dataset I found satisfied all my needs; 2) the dataset is importable to R, and is cleanable. ",State government website,"1) the dataset I found satisfied all my needs (i.e., by using the dataset, I can answer my research question, and the datasets address my independent variables and dependent variables); 2) the dataset is extractable online, importable to R, and is cleanable. ","Source reputation, Data collection conditions, Prevalence of missing data, Ability to access full datasets, Sample data",Yes,Yes,Yes,"I will look at how the datasets can fulfill my measurement need for my independent, dependent, control variables. ","I will look at: 1) is there a lot of missing data that I have to use imputation? 2) Is the data executable (e.g., importable to R, etc). ",Both (a) and (b) determine this. ,"The data does not satisfy my measurement need for variables. The data quality is poor, in a way that (1) I could not extract the data and make it importable/executable in R. ","Other sources, including accessible datasets that I can download, or do experiments. ",The same answer I had in previous questions. ,"Source reputation, Data collection conditions, Prevalence of missing data, Ability to access full datasets",Yes,Yes,Yes,The same way as I answered the previous question. ,The same way as I answered the previous question. ,The same way as I answered the previous question. 
"I found a dataset containing the posts and information like username, number of upvotes for a subreddit from Kaggle. I used this dataset to scrap the visualizations using the RedditAPI. ",I obtained the dataset from Kaggle. ,"I was looking to pull posts from a subreddit. Pulling it from a scratch would require a lot of time and playing around with Reddit API. So, instead of this, I looked online if anyone has already pulled the data for me. ","Source reputation, Metadata, Data collection conditions, Ability to access full datasets",Yes,Yes,Didn't need to pay,"Yes, the dataset I was looking fulfilled my purpose to pulling the images from the posts. I need a dataset that contained the link to the attached image and its metadata like number of upvotes, number of comments, and username. ","Yes, the dataset was of sufficient quality. ","Although I didn't need to pay, we would paid to access the data if it was not available or require some fees. ","For the same project mentioned before, I found a couple of datasets that did not have sufficient information, especially the respective metadata of the posts. There were some publicly available datasets that did not have complete information. ","I obtained these datasets prior to finding the relvant from the same source, Kaggle. ",The Kaggle shows a sample of the dataset however it doesn't show the sample if the dataset is of large size. ,"Source reputation, Metadata, Data collection conditions, Ability to access full datasets",No,Yes,Didn't need to pay,"No, the dataset did not fulfill the purpose since the datasets did not have complete information about the subreddit posts. ","No, it was not of sufficient quality. ",No